{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "np.random.seed(1237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kruthika\\Desktop\\insofe\\cute 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train-1546603042473.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('UNKNOWN', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories    0\n",
       "converse      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories    0\n",
       "converse      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_cat = data.categories.unique()\n",
    "len(uniq_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .8)\n",
    "\n",
    "train_converse = data['converse'][:train_size]\n",
    "train_cat = data['categories'][:train_size]\n",
    "\n",
    " \n",
    "test_converse = data['converse'][train_size:]\n",
    "test_cat = data['categories'][train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token with digits should be handled\n",
    "vocab_size = 15000\n",
    "num_labels = 21\n",
    "batch_size = 32\n",
    "tokenizer = Tokenizer(num_words = vocab_size,split=' ')\n",
    "tokenizer.fit_on_texts(train_converse)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_converse, mode='tfidf')\n",
    "\n",
    "x_test = tokenizer.texts_to_matrix(test_converse, mode='tfidf')\n",
    "\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_cat)\n",
    "y_train = encoder.transform(train_cat)\n",
    "y_test = encoder.transform(train_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                10773     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 7,953,941\n",
      "Trainable params: 7,953,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35063 samples, validate on 3896 samples\n",
      "Epoch 1/30\n",
      "35063/35063 [==============================] - 82s 2ms/step - loss: 1.1209 - acc: 0.6615 - val_loss: 0.8864 - val_acc: 0.7043\n",
      "Epoch 2/30\n",
      "35063/35063 [==============================] - 74s 2ms/step - loss: 0.5464 - acc: 0.8209 - val_loss: 0.9560 - val_acc: 0.7097\n",
      "Epoch 3/30\n",
      "35063/35063 [==============================] - 74s 2ms/step - loss: 0.2730 - acc: 0.9086 - val_loss: 1.1821 - val_acc: 0.7007\n",
      "Epoch 4/30\n",
      "35063/35063 [==============================] - 75s 2ms/step - loss: 0.1619 - acc: 0.9468 - val_loss: 1.3810 - val_acc: 0.7030\n",
      "Epoch 5/30\n",
      "35063/35063 [==============================] - 74s 2ms/step - loss: 0.1184 - acc: 0.9631 - val_loss: 1.5839 - val_acc: 0.6964\n",
      "Epoch 6/30\n",
      "35063/35063 [==============================] - 74s 2ms/step - loss: 0.0951 - acc: 0.9710 - val_loss: 1.6799 - val_acc: 0.6989\n",
      "Epoch 7/30\n",
      "35063/35063 [==============================] - 74s 2ms/step - loss: 0.0883 - acc: 0.9715 - val_loss: 1.7253 - val_acc: 0.6925\n",
      "Epoch 8/30\n",
      "35063/35063 [==============================] - 77s 2ms/step - loss: 0.0792 - acc: 0.9746 - val_loss: 1.8760 - val_acc: 0.6961\n",
      "Epoch 9/30\n",
      "35063/35063 [==============================] - 79s 2ms/step - loss: 0.0725 - acc: 0.9774 - val_loss: 2.0548 - val_acc: 0.6761\n",
      "Epoch 10/30\n",
      "35063/35063 [==============================] - 93s 3ms/step - loss: 0.0705 - acc: 0.9770 - val_loss: 2.0666 - val_acc: 0.6899\n",
      "Epoch 11/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0620 - acc: 0.9801 - val_loss: 2.1478 - val_acc: 0.6884\n",
      "Epoch 12/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0598 - acc: 0.9820 - val_loss: 2.2439 - val_acc: 0.6889\n",
      "Epoch 13/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0615 - acc: 0.9819 - val_loss: 2.2375 - val_acc: 0.6869\n",
      "Epoch 14/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0536 - acc: 0.9838 - val_loss: 2.2652 - val_acc: 0.6894\n",
      "Epoch 15/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0532 - acc: 0.9845 - val_loss: 2.3383 - val_acc: 0.6922\n",
      "Epoch 16/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0538 - acc: 0.9833 - val_loss: 2.3142 - val_acc: 0.6861\n",
      "Epoch 17/30\n",
      "35063/35063 [==============================] - 95s 3ms/step - loss: 0.0518 - acc: 0.9837 - val_loss: 2.4204 - val_acc: 0.6838\n",
      "Epoch 18/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0523 - acc: 0.9845 - val_loss: 2.4246 - val_acc: 0.6869\n",
      "Epoch 19/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0487 - acc: 0.9851 - val_loss: 2.4404 - val_acc: 0.6861\n",
      "Epoch 20/30\n",
      "35063/35063 [==============================] - 94s 3ms/step - loss: 0.0457 - acc: 0.9866 - val_loss: 2.4886 - val_acc: 0.6871\n",
      "Epoch 21/30\n",
      "35063/35063 [==============================] - 95s 3ms/step - loss: 0.0434 - acc: 0.9881 - val_loss: 2.5314 - val_acc: 0.6920\n",
      "Epoch 22/30\n",
      "35063/35063 [==============================] - 97s 3ms/step - loss: 0.0435 - acc: 0.9869 - val_loss: 2.5727 - val_acc: 0.6848\n",
      "Epoch 23/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0421 - acc: 0.9882 - val_loss: 2.6245 - val_acc: 0.6897\n",
      "Epoch 24/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0405 - acc: 0.9884 - val_loss: 2.6021 - val_acc: 0.6948\n",
      "Epoch 25/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0448 - acc: 0.9873 - val_loss: 2.6160 - val_acc: 0.6966\n",
      "Epoch 26/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0415 - acc: 0.9885 - val_loss: 2.6307 - val_acc: 0.6887\n",
      "Epoch 27/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0438 - acc: 0.9877 - val_loss: 2.6014 - val_acc: 0.6910\n",
      "Epoch 28/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0364 - acc: 0.9900 - val_loss: 2.6211 - val_acc: 0.6925\n",
      "Epoch 29/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0312 - acc: 0.9905 - val_loss: 2.6646 - val_acc: 0.6987\n",
      "Epoch 30/30\n",
      "35063/35063 [==============================] - 92s 3ms/step - loss: 0.0290 - acc: 0.9919 - val_loss: 2.7215 - val_acc: 0.6946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label:SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)\n",
      "Predicted label: SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)\n",
      "Actual label:PROVIDER\n",
      "Predicted label: PROVIDER\n",
      "Actual label:NEW APPOINTMENT\n",
      "Predicted label: NEW APPOINTMENT\n",
      "Actual label:REFILL\n",
      "Predicted label: REFILL\n",
      "Actual label:OTHERS\n",
      "Predicted label: OTHERS\n",
      "Actual label:MEDICATION RELATED\n",
      "Predicted label: REFILL\n",
      "Actual label:SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)\n",
      "Predicted label: PROVIDER\n",
      "Actual label:PROVIDER\n",
      "Predicted label: PROVIDER\n",
      "Actual label:MEDICATION RELATED\n",
      "Predicted label: REFILL\n",
      "Actual label:OTHERS\n",
      "Predicted label: OTHERS\n"
     ]
    }
   ],
   "source": [
    "#score = model.evaluate(x_test, y_test,batch_size=batch_size, verbose=1)\n",
    "#'''\n",
    "#print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    " \n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    print('Actual label:' + test_cat.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)\n",
    "#''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Softwares\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    }
   ],
   "source": [
    "model.model.save('my_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Tokenizer i.e. Vocabulary\n",
    "with open('tokenizer_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CANCELLATION', 'CHANGE OF HOSPITAL', 'CHANGE OF PHARMACY',\n",
       "       'CHANGE OF PROVIDER', 'FOLLOW UP ON PREVIOUS REQUEST', 'JUNK',\n",
       "       'LAB RESULTS', 'MEDICATION RELATED', 'NEW APPOINTMENT', 'OTHERS',\n",
       "       'PRIOR AUTHORIZATION', 'PROVIDER', 'QUERIES FROM INSURANCE FIRM',\n",
       "       'QUERIES FROM PHARMACY', 'QUERY ON CURRENT APPOINTMENT', 'REFILL',\n",
       "       'RESCHEDULING', 'RUNNING LATE TO APPOINTMENT',\n",
       "       'SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)',\n",
       "       'SHARING OF LAB RECORDS (FAX, E-MAIL, ETC.)', 'SYMPTOMS'],\n",
       "      dtype='<U45')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "np.random.seed(1237)\n",
    "from keras.models import load_model\n",
    "model = load_model('my_model_1.h5')\n",
    " \n",
    "# load tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "with open('tokenizer_1.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "\n",
    "encoder.classes_ #LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
